# Gemini検索エージェント (Self-Correcting Research Agent)

## 概要

これは、単なる検索ツールではありません。Googleの最新AIであるGeminiを搭載し、**自ら思考戦略を立て、自己評価と反復検索を繰り返す**ことで、ユーザーの要求に対して粘り強く、質の高い回答を生成することを目指す高度な検索エージェントです。

このプロジェクトは、AIロジックを担う「専門エージェント」 (`gemini_agent.py`) と、ユーザーとの窓口となる「司令塔」 (`main_app.py`) に役割を完全に分離した、クリーンなアーキテクチャを採用しています。

## 主な特徴

-   **🧠 自律的な戦略プランニング**:
    ユーザーの質問を分析し、「思考言語（英語/日本語）」と「難易度（高/中/低）」をAIが自ら判断。最適なアプローチで調査を開始します。

-   **🔄 自己評価と反復検索**:
    一度の検索で得られた情報が不十分な場合、エージェントは「何が足りないか」を自己評価し、より具体的なクエリで**追加のWeb検索を自動的に実行**します。これにより、情報の網羅性と精度を極限まで高めます。

-   **🏢 関心の分離アーキテクチャ**:
    AIの複雑なロジックはすべて`gemini_agent.py`にカプセル化。司令塔である`main_app.py`は、エージェントに仕事を依頼し、報告書を受け取るだけなので、コードの見通しが良く、メンテナンスや機能拡張が容易です。

-   **🔍 透明性の確保**:
    最終的な回答だけでなく、AIがどのように考え、調査を進めたかの「思考の要約」と、回答の根拠となった「情報源のURL」を明示。AIの判断プロセスを追跡できます。

## 技術スタック

-   Python 3.8以上
-   **Google AI Python SDK (`google-genai`)**
-   Requests (情報源URLの解決用)
-   Python-Dotenv (APIキー管理用)

---

## 🚀 セットアップ手順

### 1. リポジトリをクローン

```bash
git clone <リポジトリのURL>
cd <リポジトリ名>
```

### 2. Python仮想環境の作成と有効化

クリーンな環境で実行するために、仮想環境を作成することを強く推奨します。

```bash
# 仮想環境を作成
python -m venv venv

# 仮想環境を有効化
# Windowsの場合
.\venv\Scripts\activate
# macOS / Linuxの場合
source venv/bin/activate
```

### 3. APIキーの設定

1.  [Google AI Studio](https://aistudio.google.com/app/apikey)にアクセスし、APIキーを取得します。
2.  プロジェクトのルートディレクトリに `.env` という名前のファイルを作成します。
3.  作成した`.env`ファイルに、取得したAPIキーを以下のように記述します。

    **.env**
    ```
    GOOGLE_API_KEY="ここにあなたのAPIキーを貼り付けてください"
    ```

### 4. 依存パッケージのインストール

`requirements.txt`ファイルに必要なライブラリがすべて記載されています。以下のコマンドで一度にインストールします。

```bash
pip install -r requirements.txt
```*(もし`uv`をお使いの場合は `uv pip install -r requirements.txt` でも同様にインストールできます)*


## 実行方法

セットアップが完了したら、ターミナルから`main_app.py`を実行します。引数として、エージェントに尋ねたい質問を文字列で渡してください。

### 実行例

**単純な質問:**
```bash
python main_app.py "日本の現在の首相は誰ですか？"
```

**専門的な調査:**
```bash
python main_app.py "米国株で、配当利回りが4%以上の銘柄を5つ、その利回りデータと共に教えて"
```

**複雑な分析:**
```bash
python main_app.py "Pythonの非同期処理について、async/awaitの基本的な使い方とメリット・デメリットをまとめて"
```

---

## 🏛️ アーキテクチャとコード解説

### `main_app.py` (司令塔)
-   **役割**: ユーザーからの指令（コマンドライン引数）を受け取り、エージェントに渡し、返ってきた報告書（結果）を見やすく表示することに特化しています。
-   AIの内部ロジックについては一切関知しません。
-   受け取った情報源のURL（リダイレクトされている場合がある）を解決し、最終的なURLを表示する機能も持ちます。

### `gemini_agent.py` (専門エージェント)
-   **役割**: このアプリケーションの頭脳です。指令を達成するための全てのAIロジックを内包します。
-   **`_decide_strategy(query)`**:【内部関数】第一エージェント。指令の性質を分析し、最適な「思考言語」と「難易度」からなる戦略を立案します。
-   **`ask_agent(query)`**:【公開関数】第二エージェントであり、唯一の公開窓口。立案された戦略に基づき、システムプロンプト（自己評価と反復検索のルール）を設定し、Google検索ツールを使って調査を実行。最終的な回答、思考の要約、情報源を生成して司令塔に報告します。

## ライセンス

このプロジェクトは [MITライセンス](LICENSE) の下で公開されています。

---

## 拡張機能: Crawl4AIによる高度なWeb探索能力の統合

このエージェントは、外部ツールを組み込むことでさらに強力になります。ここでは、AI連携に特化した高機能Webクローリングライブラリ`Crawl4AI`を統合し、エージェントに「自らWebを深く探索し、情報を構造化して理解する能力」を付与する手順を解説します。

### 統合のメリット

- **深い情報抽出**: 単純なWeb検索だけでなく、特定のページの情報を構造化し、必要な部分だけをインテリジェントに抽出できます。
- **動的サイトへの対応**: JavaScriptでレンダリングされる現代的なWebサイトや、無限スクロールを持つページにも対応可能になります。
- **分析的なタスクの実行**: 「このページの主要な論点を3つ抜き出して」といった、より分析的なタスクが可能になります。

### 統合のための手順

`Crawl4AI`をGeminiエージェントのツールとして組み込むには、大きく分けて3つのステップが必要です。

#### ステップ1：`Crawl4AI`を呼び出す「ツール関数」の作成

まず、`gemini_agent.py`が直接呼び出せるように、`Crawl4AI`の機能をラップしたPython関数を作成します。この関数が、Geminiモデルに対する「道具」の定義そのものになります。

1.  **関数の定義**:
    -   `gemini_agent.py`内、あるいは別のファイル（例: `crawling_tool.py`）に、新しい非同期関数を定義します。例えば、`async def intelligent_web_crawl(url: str, query: str)` のような形です。
    -   この関数は、Geminiモデルが理解できるような単純な引数（URL、検索クエリなど）を受け取るように設計します。

2.  **`Crawl4AI`の呼び出し**:
    -   関数内部で、`Crawl4AI`の`AsyncWebCrawler`をインスタンス化します。
    -   ユーザーからの`query`（質問）を基に、`Crawl4AI`の高度な抽出・フィルタリング戦略を設定します。例えば、`BM25ContentFilter`を使って、`query`に関連する部分だけを抽出するように設定します。
    -   `crawler.arun(url=url, ...)` を実行して、実際にクローリングを行います。

3.  **結果の整形**:
    -   `Crawl4AI`からの戻り値（`CrawlResult`オブジェクト）は非常に情報量が多いため、そのままではLLMが扱いにくい場合があります。
    -   `markdown`や`fit_markdown`、抽出したテーブル（`tables`）など、最も重要で要約された情報だけを取り出し、LLMが理解しやすいシンプルなテキスト形式やJSON形式に整形して返します。

#### ステップ2：作成したツールをGeminiエージェントに登録

次に、ステップ1で作成した関数を、`ask_agent`関数内でGeminiモデルが利用可能なツールとして登録します。

1.  **ツール定義のインポート**:
    -   `gemini_agent.py`の`ask_agent`関数内で、ステップ1で作成した`intelligent_web_crawl`関数をインポートします。

2.  **ツールリストへの追加**:
    -   `ask_agent`関数の中にある`tools_to_use`というリストを探します。
    -   現在登録されている`GoogleSearch`や`UrlContext`に加えて、`Crawl4AI`を呼び出すための新しいツール定義を追加します。
    -   これは `types.Tool` を使って行い、関数のスキーマ（関数名、説明、引数）をモデルに教えます。説明文は「特定のURLを深く分析し、ユーザーの質問に関連する部分だけを構造化して抽出します」のように、モデルがツールの役割を理解しやすいように記述するのが重要です。

#### ステップ3：エージェントのプロンプト（システム指示）の強化

最後に、エージェントが新しいツールを**いつ、どのように使うべきか**を理解できるように、`system_instruction`を更新します。

1.  **行動ルールの追加**:
    -   `ask_agent`関数内の`system_instruction`（特に`thinking_language == 'english'`の場合のプロンプト）に、新しいルールを追加します。
    -   例えば、「Google検索で見つけたURLが、ユーザーの質問に答えるための重要な情報を含んでいると判断した場合、`intelligent_web_crawl`ツールを使ってそのページを深く分析しなさい」といった指示を加えます。
